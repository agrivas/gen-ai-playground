{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Mistral-7B-Instruct-v0.2 to process medical transcripts\n",
    "Based on https://www.kaggle.com/code/malenaprezsevilla/introduction-to-prompt-engineering-using-mistral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model setup\n",
    "Quantization and Accelerate allow inference to run even on a laptop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11997/2422686872.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n"
     ]
    }
   ],
   "source": [
    "# General packages\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from textwrap import fill\n",
    "from IPython.display import Markdown, display # for formating Python display folowing markdown language\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # avoid warning messages importing packages\n",
    "\n",
    "# Mistral and LangChain packages (prompt engineering)\n",
    "import torch\n",
    "from langchain import PromptTemplate, HuggingFacePipeline\n",
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:04<00:00, 21.63s/it]\n"
     ]
    }
   ],
   "source": [
    "# Model version of Mistral\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# Quantization is a technique used to reduce the memory and computation requirements \n",
    "# of deep learning models, typically by using fewer bits, 4 bits\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Initialization of a tokenizer for the Mistral-7b model, \n",
    "# necessary to preprocess text data for input\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Initialization of the pre-trained language Mistral-7b\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "\n",
    "# Configuration of some generation-related settings\n",
    "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
    "generation_config.max_new_tokens = 1024 # maximum number of new tokens that can be generated by the model\n",
    "generation_config.temperature = 0.7 # randomness of the generated tex\n",
    "generation_config.top_p = 0.95 # diversity of the generated text\n",
    "generation_config.do_sample = True # sampling during the generation process\n",
    "generation_config.repetition_penalty = 1.15 # the degree to which the model should avoid repeating tokens in the generated text\n",
    "\n",
    "# A pipeline is an object that works as an API for calling the model\n",
    "# The pipeline is made of (1) the tokenizer instance, the model instance, and\n",
    "# some post-procesing settings. Here, it's configured to return full-text outputs\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,\n",
    "    generation_config=generation_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace pipeline\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model\n",
    "Instruction format: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2#instruction-format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>Explain the fundamentals of ChatGPT in a couple of lines.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "ChatGPT is a type of large language model developed by OpenAI and currently operated by Microsoft. It uses deep learning techniques to understand input text, generate human-like responses, and maintain a conversation with users. The model's ability to comprehend context and provide relevant information makes it suitable for various applications like customer service, education, and entertainment. However, keep in mind that while advanced, it doesn't have access to personal data or real-world knowledge unless explicitly provided during interaction."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generate(model, text, template=None, format_instructions=None):\n",
    "    if template == None:\n",
    "        template = \"[INST]{text}[/INST]\"\n",
    "    \n",
    "    prompt = PromptTemplate.from_template(template)\n",
    "    \n",
    "    response = model(prompt.format(text = text, format_instructions = format_instructions))\n",
    "    return response.strip()\n",
    "\n",
    "def generate_and_display(model, text, template=None, format_instructions=None):\n",
    "    result = generate(model, text, template, format_instructions)\n",
    "\n",
    "    # No point displaying a templated prompt, this is just a convenience for simple prompts\n",
    "    if (template == None):\n",
    "        display(Markdown(f\"<b>{text}</b>\"))\n",
    "\n",
    "    display(Markdown(f\"{result}\"))\n",
    "\n",
    "generate_and_display(llm, \"Explain the fundamentals of ChatGPT in a couple of lines.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medical transcripts dataset\n",
    "Source: https://www.kaggle.com/datasets/tboyle10/medicaltranscriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>medical_specialty</th>\n",
       "      <th>sample_name</th>\n",
       "      <th>transcription</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A 23-year-old white female presents with comp...</td>\n",
       "      <td>Allergy / Immunology</td>\n",
       "      <td>Allergic Rhinitis</td>\n",
       "      <td>SUBJECTIVE:,  This 23-year-old white female pr...</td>\n",
       "      <td>allergy / immunology, allergic rhinitis, aller...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Consult for laparoscopic gastric bypass.</td>\n",
       "      <td>Bariatrics</td>\n",
       "      <td>Laparoscopic Gastric Bypass Consult - 2</td>\n",
       "      <td>PAST MEDICAL HISTORY:, He has difficulty climb...</td>\n",
       "      <td>bariatrics, laparoscopic gastric bypass, weigh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Consult for laparoscopic gastric bypass.</td>\n",
       "      <td>Bariatrics</td>\n",
       "      <td>Laparoscopic Gastric Bypass Consult - 1</td>\n",
       "      <td>HISTORY OF PRESENT ILLNESS: , I have seen ABC ...</td>\n",
       "      <td>bariatrics, laparoscopic gastric bypass, heart...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2-D M-Mode. Doppler.</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>2-D Echocardiogram - 1</td>\n",
       "      <td>2-D M-MODE: , ,1.  Left atrial enlargement wit...</td>\n",
       "      <td>cardiovascular / pulmonary, 2-d m-mode, dopple...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2-D Echocardiogram</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>2-D Echocardiogram - 2</td>\n",
       "      <td>1.  The left ventricular cavity size and wall ...</td>\n",
       "      <td>cardiovascular / pulmonary, 2-d, doppler, echo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  \\\n",
       "0   A 23-year-old white female presents with comp...   \n",
       "1           Consult for laparoscopic gastric bypass.   \n",
       "2           Consult for laparoscopic gastric bypass.   \n",
       "3                             2-D M-Mode. Doppler.     \n",
       "4                                 2-D Echocardiogram   \n",
       "\n",
       "             medical_specialty                                sample_name  \\\n",
       "0         Allergy / Immunology                         Allergic Rhinitis    \n",
       "1                   Bariatrics   Laparoscopic Gastric Bypass Consult - 2    \n",
       "2                   Bariatrics   Laparoscopic Gastric Bypass Consult - 1    \n",
       "3   Cardiovascular / Pulmonary                    2-D Echocardiogram - 1    \n",
       "4   Cardiovascular / Pulmonary                    2-D Echocardiogram - 2    \n",
       "\n",
       "                                       transcription  \\\n",
       "0  SUBJECTIVE:,  This 23-year-old white female pr...   \n",
       "1  PAST MEDICAL HISTORY:, He has difficulty climb...   \n",
       "2  HISTORY OF PRESENT ILLNESS: , I have seen ABC ...   \n",
       "3  2-D M-MODE: , ,1.  Left atrial enlargement wit...   \n",
       "4  1.  The left ventricular cavity size and wall ...   \n",
       "\n",
       "                                            keywords  \n",
       "0  allergy / immunology, allergic rhinitis, aller...  \n",
       "1  bariatrics, laparoscopic gastric bypass, weigh...  \n",
       "2  bariatrics, laparoscopic gastric bypass, heart...  \n",
       "3  cardiovascular / pulmonary, 2-d m-mode, dopple...  \n",
       "4  cardiovascular / pulmonary, 2-d, doppler, echo...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('../data/medical-transcripts/mtsamples.csv',index_col=0)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>medical_specialty</th>\n",
       "      <th>sample_name</th>\n",
       "      <th>transcription</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4999</td>\n",
       "      <td>4999</td>\n",
       "      <td>4999</td>\n",
       "      <td>4966</td>\n",
       "      <td>3931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2348</td>\n",
       "      <td>40</td>\n",
       "      <td>2377</td>\n",
       "      <td>2357</td>\n",
       "      <td>3849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>An example/template for a routine normal male...</td>\n",
       "      <td>Surgery</td>\n",
       "      <td>Lumbar Discogram</td>\n",
       "      <td>PREOPERATIVE DIAGNOSIS: , Low back pain.,POSTO...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>12</td>\n",
       "      <td>1103</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              description medical_specialty  \\\n",
       "count                                                4999              4999   \n",
       "unique                                               2348                40   \n",
       "top      An example/template for a routine normal male...           Surgery   \n",
       "freq                                                   12              1103   \n",
       "\n",
       "               sample_name                                      transcription  \\\n",
       "count                 4999                                               4966   \n",
       "unique                2377                                               2357   \n",
       "top      Lumbar Discogram   PREOPERATIVE DIAGNOSIS: , Low back pain.,POSTO...   \n",
       "freq                     5                                                  5   \n",
       "\n",
       "       keywords  \n",
       "count      3931  \n",
       "unique     3849  \n",
       "top              \n",
       "freq         81  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and show the transcription of one record\n",
    "def show_transcription(df,i,width=50):\n",
    "    print('Record %i\\n' %i)\n",
    "    text = (Markdown(f\"<p>{df.loc[i,'transcription']}</p>\"))\n",
    "    \n",
    "    #for l in textwrap.wrap(text=df.loc[i,'transcription'],width=width):\n",
    "    #    print(l)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PHYSICAL EXAMINATION: , The patient is a 63-year-old executive who was\n",
      "seen by his physician for a company physical.  He stated that he was\n",
      "in excellent health and led an active life.  His physical examination\n",
      "was normal for a man of his age.  Chest x-ray and chemical screening\n",
      "blood work were within normal limits.  His PSA was\n",
      "elevated.,IMAGING:,Chest x-ray:  Normal.,CT scan of abdomen and\n",
      "pelvis:  No abnormalities.,LABORATORY:,  PSA 14.6.,PROCEDURES: ,\n",
      "Ultrasound guided sextant biopsy of prostate:  Digital rectal exam\n",
      "performed at the time of the biopsy showed a 1+ enlarged prostate with\n",
      "normal seminal vesicles.,PATHOLOGY:  ,Prostate biopsy:  Left apex:\n",
      "adenocarcinoma, moderately differentiated, Gleason's score 3 + 4 =\n",
      "7/10.  Maximum linear extent in apex of tumor was 6 mm.  Left mid\n",
      "region prostate:  moderately differentiated adenocarcinoma, Gleason's\n",
      "3 + 2 = 5/10.  Left base, right apex, and right mid-region and right\n",
      "base:  negative for carcinoma.,TREATMENT:,  The patient opted for low\n",
      "dose rate interstitial prostatic implants of I-125.  It was performed\n",
      "as an outpatient on 8/10.\n"
     ]
    }
   ],
   "source": [
    "texto=df.loc[60,'transcription']\n",
    "print(fill(texto))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"$defs\": {\"GenderEnum\": {\"enum\": [\"Male\", \"Female\"], \"title\": \"GenderEnum\", \"type\": \"string\"}, \"ImagingSchema\": {\"properties\": {\"type\": {\"description\": \"The type of medical imaging performed\", \"title\": \"Type\", \"type\": \"string\"}, \"findings\": {\"description\": \"The findings of the medical imaging performed\", \"title\": \"Findings\", \"type\": \"string\"}}, \"required\": [\"type\", \"findings\"], \"title\": \"ImagingSchema\", \"type\": \"object\"}}, \"properties\": {\"age\": {\"anyOf\": [{\"type\": \"integer\"}, {\"type\": \"null\"}], \"description\": \"Patient's age\", \"title\": \"Age\"}, \"gender\": {\"anyOf\": [{\"$ref\": \"#/$defs/GenderEnum\"}, {\"type\": \"null\"}], \"description\": \"Patient's gender\"}, \"imaging\": {\"description\": \"The list of medical imaging examinations performed\", \"items\": {\"$ref\": \"#/$defs/ImagingSchema\"}, \"title\": \"Imaging\", \"type\": \"array\"}}, \"required\": [\"age\", \"gender\", \"imaging\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "from enum import Enum\n",
    "\n",
    "class GenderEnum(Enum):\n",
    "    Male = 'Male'\n",
    "    Female = 'Female'\n",
    "\n",
    "class ImagingSchema(BaseModel):\n",
    "    type: str = Field(..., description=\"The type of medical imaging performed\")\n",
    "    findings: str = Field(..., description=\"The findings of the medical imaging performed\")\n",
    "\n",
    "class PatientSchema(BaseModel):\n",
    "    age: Optional[int] = Field(..., description=\"Patient's age\")\n",
    "    gender: Optional[GenderEnum] = Field(..., description=\"Patient's gender\")\n",
    "    imaging: List[ImagingSchema] = Field(..., description=\"The list of medical imaging examinations performed\")\n",
    "\n",
    "# Instance of the parser\n",
    "output_parser = PydanticOutputParser(pydantic_object=PatientSchema)\n",
    "\n",
    "# This is what will be added to the prompt in order to get the model to respond in JSON\n",
    "print(output_parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "{\n",
       "  \"age\": 63,\n",
       "  \"gender\": \"Male\",\n",
       "  \"imaging\": [\n",
       "    {\n",
       "      \"type\": \"chest x-ray\",\n",
       "      \"findings\": \"Normal\"\n",
       "    },\n",
       "    {\n",
       "      \"type\": \"ct scan of abdomen and pelvis\",\n",
       "      \"findings\": \"No abnormalities\"\n",
       "    }\n",
       "  ]\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "template = \"\"\"[INST]\n",
    "You are a medical expert reading through transcripts. Your expertise spans the whole medical domain.\n",
    "\n",
    "Your job is to extract structured information from the transcript.\n",
    "\n",
    "This is the transcript: ```{text}```\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "Your response JSON should only include the fields defined in the schema above. Ignore all other information from the transcript.\n",
    "\n",
    "Respond only with valid JSON, add no further comments to the response.\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "generate_and_display(llm, texto, template, output_parser.get_format_instructions())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
